CKA 20-6-20
-------------------------
Cluster Arcitecture:
-  We have 2 kind of ships here
1. Cargo Ships -  Doing the Actual work by carring the containers.
2. Controll Ship - Resposible for monitoring & managing the cargo ships.

Worker Nodes (Cargo Ships) :-
- The WOrker Nodes in the cluster are ships that can load contaiers.
- 



Controll Ships - 
- To load the containers on the Cargo ships and not just load plan how to load identify the 
right ship & plan to load.
Store information about the ships monitor and track the location of containers on the
ships.
- Manage the whole loading process etc.
- THis is done by the control ships that hist different offices and 
departments monitoring equipments, Communication equipments, cranes for moving containers between
ships etc. 
- The control ship relates to the master node in the Kubernetes Cluster the master node 
is responsible for managing the kubernetes.
- The Master Node is responsible for managing the kubernetes 
1. Storig information regarding the different nodes planning which contaiers
cause where monitoring the nodes and contaierson them etc.
- Master node manage this with help of some components,
1. ETCD cluster :
- Stores all the information about the load & unload of containers of nodes.
- The informationof nodes & what contaiers is on which ship and what time it was loaded
- All this is available in a "key:value" store.
2. Kube Scheduler : 
- To load containers on ships, we need crains & the crains identify the cntainers that need
to place on ships.
- Identify the right ship based on size its capacity the number of contaiers already on the 
on the ship & any other ocndition such as the destination of the ship.
- The type of contaiers it is allowed to carry etc.
- It basically identifies the right node to place a contaier on based on a contaiers resource 
Requirement the Worker nodes capacity or any other policies or constraints such as tents & tolerance
or node afinity rules that are on them.
3. Controller-Manager:
- Node-Controller : He is responsible for on-boarding new nodes to cluster, Handling situation
where nodes become unavailable or gets destroyed.
- Replication Controller : Replication controlloer ensures
that the desired number of containers are running at all times in your replication group.
4. Kube APIserver:
- The kube-apiserver is the primary management component of kubernetes.
- The kube-apiserver is responsible for orchestrating all operations within the cluster.
- It exposes the K8's API which is used by extwernal users to perform management operations
on the cluster as well as various controllers to monitor the state of the cluster and make
the necessary as required and by the worker node to communicate with the server. 

Working Node:
Now every ship has a captain.
The captain is responsible for managing all activities on these ships.
The captain is responsible for liaising with the master ships starting with letting the master ship
know that they are interested in joining the group receiving information about the containers to be
loaded on the ship and loading the appropriate containers as required sending reports back to the master
about the status of this ship and the status of the containers on the ship etc.
Now the captain of the ship is the kubelet in Kubernetes. A kubelet is an agent that runs on
each node in a cluster.

1. Kubelet: It is running on each node in a cluster
- It listens for instructions from the "kube-apiserver" & deploys or destroys containers
on the nodes as required.
- The kube-api server periodically fetches status reports from the kubelet to monitor the state of nodes
and containers on them.
2. Kube-Proxy: 
- The kubelet was more of a captain on the ship that manages containers on the ship. But the applications
running on the worker notes need to be able to communicate with each other.
--- For example you might have a web server running in one container on one of the nodes and a database
server running on another container.
on another node. How would the web server reach the database server on the other node.---
- Communication between worker nodes are enabled by another component that runs on the worker node known
as the Kube-proxy service. 
- The Kube-proxy service ensures that the necessary rules are in place on the worker nodes to allow the 
containers running on them to reach each other.



## So to summarize we have master and worker nodes on the master.
- we have the ETCD cluster which stores information about the cluster
- we have the Kube scheduler that is responsible for scheduling applications or containers on Nodes
- We have different controllers that take care of different functions like the node control, replication,
controller etc..
- we have the Kube api server that is responsible for orchestrating all operations within the cluster.
on the worker node.
- we have the kubelet that listens for instructions from the Kube-apiserver and manages containers and
- The kube-proxy,That helps in enabling communication between services within the cluster.


## ETCD
- It is a distributed reliable key value store that is simple secure and fast.
- When you run ETCD it starts a service that listens on PORT- 2379 by default.
- "etcdtl" it is client for entity you can use it to store and retrieve key value pairs.
- To store info "etcdctl set key1 value1" to store information,
To retrive data "etcdctl get key1 value1"

## ETCD in Kubernetes
- The ETCD datastore stores information regarding the
cluster such as the nodes, pods, configs, secrets, accounts, roles, bindings and others.
- Every information you see when you run the kubectl get command is from the ETCD server. 
- Every change you make to your cluster, such as adding additional nodes, deploying pods or replica sets are updated in the ETCD server. Only once it is updated in the ETCD server, is the change considered to be complete.
- the advertised client url.
This is the address on which ETCD listens. It happens to be on the IP of the server and on port 2379, which is the default port on which etcd listens.
- This is the URL that should be configured on the "kube-api server" when it tries to reach the etcd server.
- Kubernetes stores data in the specific directory structure the root directory is a registry and under that you have the various kubernetes constructs such as minions or nodes, pods, replicasets, deployments etc.
-  In a high availability environment you will have multiple master nodes in your cluster then you will have multiple ETCD instances spread across the master nodes.

In that case, make sure to specify the ETCD instances know about each other by setting the right parameter in the ETCD service configuration. The initial-cluster option is where you must specify the different instances of the ETCD service.
"etcd.service
ExecStart=/usr/local/bin/etcd
--initial-cluster controller-o=//${CONTROLLER0_IP}:2380,controller-1=https://${CONTROLLER1_IP}:2380 \\"

## kube-api server
- The Kube-api server is the primary management component in kubernetes.
- When you run a kubectl command, the kubectl utility is infact reaching to the kube-apiserver.
- The kube-api server first authenticates the request and validates it. It then retrieves the data from the ETCD cluster and responds back with the requested information.
- To summarize, the kube-api server is responsible for Authenticating and validating requests, retrieving and updating data in ETCD data store.
in fact, kube-api server is the only component that interacts directly with the etcd datastore.
- The other components such as the scheduler, kube-controller-manager & kubelet uses the API server to perform updates in the cluster in their respective areas.
- If you bootstrapped your cluster using kubeadm tool then you don't need to know this but if you are setting up the hard way, then kube-apiserver is available as a binary in the kubernetes release page. 
- The kube-api server is run with a lot of parameters as you can see here. 
- The kubernetes architecture consists of a lot of different components working with each other, talking to each other in many different ways so they all need to know where the other components, There are different modes of authentication, authorization, encryption and security. And that’s why you have so many options .
- "ExecStart=/usr/local/bin/kube-apiserver" -- To check the components of kube-api server.
- There is a option in the list of components, where you can find out
ETCD-servers is where you specify the location of the ETCD servers.
- This is how the kube-api server connects to the etcd servers.
- . If you set it up with kubeadm tool, kubeadm deploys the kube-api server as a pod in the kube-system namespace on the master node you can see the options within the pod definition file located at /etc/kubernetes/manifests/kube-apiserver.yaml folder.
- kube-apiserver service located at /etc/systemd/system/kube-apiserver.service.You can also see the running process and the effective options by listing the process on the master node and searching for kube-apiserver.
- You can see the running process and the effective options by listing the process on the master node and searching "ps -aux | grep kube-apiserver"

## Kube Controller Manager
- the kube controller manager manages various controllers in Kubernetes. A controller is like an office or department within the master ship that have their own set of responsibilities. 
- Such as an office for the Ships would be responsible for monitoring and taking necessary actions about the ships.
- Whenever a new ship arrives or when a ship leaves or gets destroyed another office could be one that manages the containers on the ships they take care of containers that are damaged or full of ships.
- So these officers are number 1, continuously on the lookout for the status of the ships and 2, take necessary actions to remediate the situation.
- In the kubernetes terms a controller is a process that continuously monitors the state of various components within the system and works towards bringing the whole system to the desired functioning state.
- For example the node controller is responsible for monitoring the status of the nodes and taking necessary actions to keep the application running.
- It does that through the kube-api server.The node controller checks the status of the nodes every 5 seconds.That way the node controller can monitor the health of the nodes if it stops receiving heartbeat from
a node the node is marked as unreachable but it waits for 40 seconds before marking it unreachable.
- after a node is marked unreachable it gives it five minutes to come back up if it doesn’t, it removes the PODs assigned to that node and provisions them on the healthy ones.
- If the PODs are part of a replica set the next controller is the replication controller.It is responsible for monitoring the status of replica sets and ensuring that the desired number of PODs are available at all times within the set.If a pod dies it creates another one.
- There are many more such controllers available within kubernetes.
Whatever concepts we have seen so far in kubernetes such as deployments, Services, namespaces,persistent volumes and whatever intelligence is built into these constructs it is implemented through these various controllers.
- They're all packaged into a single process known as kubernetes controller manager.When you install the kubernetes controller manager the different controllers get installed as well.So how do you install and view the kubernetes Controller manager download the kube-controller-manager from the kubernetes release page. Extract it and run it as a service.
- When you run it as you can see there are a list of options provided this is where you provide additional options to customize your controller.
- Remember some of the default settings for node controller we discussed earlier such as the node monitor period the grace period and the eviction timeout.These go in here as options.
- There is an additional option called controllers that you can use to specify which controllers to enable.By default all of them are enabled but you can choose to enable a select few.
- So how do you view the Kube-controller-manager server options?
Again it depends on how you set up your cluster.If you set it up with kubeadm tool, kubeadm deploys the kube-controller-manager as a pod in the
kube-system namespace on the master node("kubectl get pods -n kube-system"). You can see the options within the pod definition file located at "etc/kubernetes/manifests/kube-controller-manager.yaml". 
- In a non-kubeadm setup, you can inspect the options by viewing this
"cat /etc/systemd/system/kube-controller-manager.service"at etc kubernetes manifests folder. In a non-kubeadm setup, you can inspect the options by viewing kube-controller-manager service located at the services directory.
- You can also see the running process and the effective options by listing the process on the master node and searching for kube-controller-manager "ps -aux | grep kube-controller-manager".

## Kube Scheduler

- Earlier we discussed that the kubernetes scheduler is responsible for scheduling pods on nodes.
- It is only responsible for deciding which pod goes on which node. It doesn’t actually place the pod on the nodes.That’s the job of the kubelet. The kubelet or the captain on the ship is who creates the pod on the ships.
- The scheduler only decides which pod goes where.
- Let's look at how the scheduler does that in a bit more detail.

First of all why do you need a scheduler? When there are many ships and many containers,

You want to make sure that the right container ends up on the right ship.

For example there could be different sizes of ships and containers.

You want to make sure the ship has sufficient capacity to accommodate those containers different ships

maybe going to different destinations.

You want to make sure your containers are placed on the right ships so they end up in the right destination.

- In kubernetes, the scheduler decides which nodes the pods are placed on
depending on certain criteria. You may have PODs with different resource requirements,You can have nodes in the cluster dedicated to certain applications.

So how does the scheduler assign these PODs? The scheduler looks at each POD and tries to find the best node for it.

- For example, let’s take one of these PODs.
pod -4 pod-4 pod-12 pod-16
The big one. It has a set of CPU and Memory requirements.

- The scheduler goes through two phases to identify the best node for the pod in the first phase.
- The scheduler tries to filter out the nodes that do not fit the profile for this pod.
- For example, the nodes that do not have sufficient CPU and memory resources requested by the pod.So the first two small nodes are filtered out. So we are now left with two nodes on which the POD can be placed.
- Now how does the scheduler pick one from the two the scheduler ranks the node to identify the best fit for the pod.
- It uses a priority function to assign a score to the nodes on a scale of 0 to 10.
- For example the scheduler calculates the amount of resources that would be free on the nodes after placing the pod on them.
- In this case, the one on the right would have 6 CPUs free if the pod was placed on it which is 4 more than the other one so it gets a better rank And so it wins.
- So that's how a scheduler works at a high level And of course these can be customized and you can write your own scheduler as well.
- There are many more topics to look at such as resource requirements, limits, taints and tolerations,node selectors, affinity rules etc. 
- So how do you install the kube-scheduler?
Download the kube-scheduler from the kubernetes release page. Extract it and run it as a service.when you run it as a service.You specify the scheduler configuration file.
So how do you view the kube-scheduler server options?
"kube-scheduler.service"
- Again, if you set it up with kubeadm tool, kubeadm deploys the kube-scheduler as a pod in the kube-system namespace on the master node. You can see the options within the pod definition file located at :
"cat /etc/kubernetes/manifests/kube-scheduler.yaml".
- You can also see the running process and the effective options by listing the process on the master node and searching for kube-scheduler
"ps -aux | grep kube-scheduler"

## Kubelet
- They're the sole point of contact from the master ship.
They load or unload containers on the ship as instructed by the scheduler on the master.
- They're the sole point of contact from the master ship.
They load or unload containers on the ship as instructed by the scheduler on the master.
- They also send back reports at regular intervals on the status of the ship and the containers on them.
- The kubelet in the kubernetes worker node, registers the node with the kubernetes cluster.When it receives instructions to load a container or a POD on the node, it requests the container run time engine, which may be Docker, to pull the required image and run an instance.
- The kubelet then continues to monitor the state of the POD and the containers in it and reports to the kube-api server on a timely basis.
- So how do you install the kubelet? If you use kubeadm tool to deploy your cluster, it does not automatically deploy the kubelet.
- You must always manually install the kubelet on your worker nodes. Download the installer, extract it and run it as a service.
- You can view the running kubelet process and the effective options by listing the process on the worker node and searching for kubelet.
"ps aux | grep kubelet"

## Kube-Proxy
- Within a kubernetes cluster every pod can reach every other pod.
This is accomplished by deploying a POD networking solution to the cluster. A POD network is an internal virtual network that spans across all the nodes in the cluster to which all the PODs connect to. 
- Through this network pods are able to communicate with each other.
There are many solutions available for deploying such a network.
- In this case I have a web application deployed on the first node and a database application deployed on the second.
- The web app can reach the database, simply by using the IP of the database POD.But there is no guarantee that the IP of the database part will always remain the same.
- The better way for the web application to access the database is using a service. So we create a service to expose the database application across the cluster.
- Now The web application can now access the database using the name of the service db irrespective of IP.
- The service also gets an IP address assigned to it whenever a pod tries to reach the service using its IP or name it forwards the traffic to the back end pod.

* But what is this service and how does it get an IP?
* Does the service join the same POD Network?

- The service cannot join the pod network because the service is not an actual thing.
- It is not a container like pod so it doesn't have any interfaces or an actively listening process.
- It is a virtual component that only lives in the kubernetes as memory.

* But then we also said that the service should be accessible across the cluster from any not.

So how is that achieved?

- That’s where kube-proxy comes in.
- Kube-proxy is a process that runs on each node in the kubernetes cluster.
- Its job is to look for new services and every time a new service is created it creates the appropriate rules on each node to forward traffic to those services to the backend pods.
- One way it does this is using IPTABLES rules.
- In this case it creates an IP tables rule on each node in the cluster to forward traffic heading to the IP of the service which is 10.96.0.12 to the IP of the actual pod which is 10.32.0.15. So how kube-proxy configure the service.

* how to install kube-proxy?
- Download the kube-proxy binary from the kubernetes release page. Extract it and run it as a service.
  "kube-proxy.service"
- kubeadm tool deploys kube-proxy as PODs on each node.
  "kubectl get pods -n kube-system"
- In fact it is deployed as a daemon set, so a single POD is always deployed on each node in the cluster.
"kubectl get daemonset -n kube-system"

## PODS_WIth_YAML
- K8's uses YAML files as inputs for the creation of objects such as parts report cards
deployment services etc
- A k8's defination file always contains four top level fields.
* apiVersion
* kind
* metadata
* spec
- These are the top level or root level properties.
- These are also required fields so you must have them in your configuration file.
* apiVersion -
- We are using to create the object depending on what we are trying to create.
- We must use the right API version for now since we are working on PODS.
We will use API version "v1"
POD = v1
Service = v1
ReplicaSet = apps/v1
Deployment = apps/v1

* Kind - 
- the kind refers to the type of object we are trying to create which in this case happens to be a POD.
- It may be some other values as such we mentioned in the above.
-

* metadata -
- It is the data about the object.Like its name, lables etc. 
- As you can see unlike the first two where you have specified a string value this is in the form of a dictionary.
- So everything under metadata is intended to the right a little bit and so names and labels are children of metadata.
- The number of spaces before the two properties name and labels doesn't matter but they should be the same as they are siblings.Example:-
* metadata:
    name: myapp-pod
    labels:
        app: myapp
- In this case labels has more spaces on the left than name and so it is now a child of the name property instead of a sibling which is incorrect also the two properties must have more spaces than its parent.

* spec:
- This is where we would provide additional information to K8's pertaining to that object.
- This is going to be different for different objects so it's important to understand or refer to the documentation section to get the right format for each.
- since we are only creating a pod with a single container in it is easy.
- spec is a dictionary so add a property under it called containers.It is a list or an array. 
-  The reason this property is a list is because the PODS can have multiple containers within them.
---------------------------------------------------------------------------
## COMMANNDS For Our Use
- For additional info about pods: k get pods -o wide
- To replace or update the replicaset : k replace -f <replicaset-defination.file>
- to scale the replicaset : k scale --replicas=6 -f <replicaset-defination.file> (without modifying the file)
- To see all the created objects at once: k get all
- To add/increase values in deployment(any sub): k scale deployment <dep-name> --replicas=3
- To switch between namespaces: k config set-context $(k config current-context) --namespace=<name>
- To list all pods of all namespace: k get pods --all-namespace
- To calculate all namespaces: k get ns --no-headers | wc -l
- To check pods on namespace : k -n <namespace> get pods
- Imperative Command for pod on a namespace: k run redis --image=redis --restart=never(to avoid the deployment) --dry-run(to create the result) -o yaml > pod.yaml
- Cretiong a new service with imperative commands : k expose deployment <dep-name> --type=NodePort --target-port=8080 --target-port=8080 --name=dev --dry-run -o yaml >pod.yaml

## Imperative Commands with kubectl
*  --dry-run: By default as soon as the command is run, the resource will be created. If you simply want to test your command, use the --dry-run=client option. This will not create the resource, instead, tell you whether the resource can be created and if your command is right.

*  -o yaml: This will output the resource definition in YAML format on the screen.
## POD
* Create an NGINX Pod : kubectl run nginx --image=nginx
* Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run) : kubectl run nginx --image=nginx  --dry-run=client -o yaml

## Deployment
* Create a deployment: kubectl create deployment --image=nginx nginx
* Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) : kubectl create deployment --image=nginx nginx --dry-run=client -o yaml
* IMPORTANT:kubectl create deployment does not have a --replicas option. You could first create it and then scale it using the kubectl scale command.
* Save it to a file - (If you need to modify or add some other details) : kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml
- You can then update the YAML file with the replicas or any other field before creating the deployment.

## Service
* Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379 : kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml  (This will automatically use the pod's labels as selectors)

Or

* kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml  (This will not use the pods labels as selectors, instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service)

* Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:

* kubectl expose pod nginx --port=80 --name nginx-service --type=NodePort --dry-run=client -o yaml (This will automatically use the pod's labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port in manually before creating the service with the pod.)

Or

* kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml
(This will not use the pods labels as selectors)

* Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port. I would recommend going with the `kubectl expose` command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.

## SCHEDULING
* Manual Scheduling
- How scheduling works?
- 

## COMMANNDS For Our Use
- To get pods with a selected label : k get pods -l env=dev | wc -l
- To get all list of services : k get all -l env=dev
- To get the pod which is in "prod" part of other services also: k get pod -l env=prod,bu=finance,tier=frontend (comma separated values)
-
## Taints & Tolerations
- Taints are always with pod & Toleration always with PODs
- How to add taint to the node :
  * kubectl taint nodes <node-name> key=value:taint-effect
  * if you would like to dedicate the node to parts in application
    blue then the key value pair will be "app=blue".
  * "taint-effect" defines what would happen to the pods.
  * if they do not tolerate the taint there are three taint effects
  no-schedule : the pod won't be scheduled on the node.
  preferNoSchedule : it means the system will try to avoid placing pod on the node but this is not guaranteed.
  NoExecute : The new pods will not be scheduled on the node and existing pods on the node if any get evicted if they don't tolerate the taint.
  these pods may have been scheduled before the taint was applied to the node.
* An example command will be - kubectl taint nodes node1 app=blue:NoSchedule
- Taints & Tolerations are only meant to be restrict nodes from accepting ceratin pods.
- It also not guarantees to a pod to give a perticular node for there work. Instead  it tells the node to only accept  pods with certain tolerations.
- The master node automatically has a taint that prevents any pods from being schedule on master.
  * To see the taint - kubectl describe node kubemaster| grep taint
  * To remove the taint form the master node - kubectl taint nodes master node-role.kubernetes.io/master:NoSchedule-

# Tolerations - PODS
- Toleartions are added to the pods.
- At first we need to pull up the pod defination file, in the "spec" add a section called "toleration"
we need to use the same values of the "taint" over here.
- tolerations:
  - key:"app"
   operator: "Equal"
   value: "blue"
   effect: "NoSchedule"
  "All this values should be on encoded in double quotes"
---------------------------------------------------------------------------------------------------------------

## Node Selector
- How to add node selecter to your pod defination file:
   spec:
    nodeSelector:
     size:Large
- we need to add the same to "key:value" pair to your node .
  kubectl label node <node-name> <label-key>:<label:value>
  kubectl label node node01 size:Large

- It serves a purpose but it has limitations. It won't be work with much more complex scenario.
* Example :- For example, we would like to say something like place the pod on a large or medium node or something
like place the pod on any nodes that are not small.
- You cannot achieve this using Node selectors. 
- Her you cannot give advance expressions like "Large or Medium" or Not small.
## Node Affinity
- The primary purpose of node affinity feature is to ensure that pods are hosted on particular nodes.
- Greate power come from great complexity.
- The Specification:
    spec:
      affinity:
       nodeAffinity:
       requireDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - MatchExpression:
        - key: size
          operator: In/Not In (it ensures the pod will be placed on a node whose label size has any value in the list of values specified)
          values:
          -Large
(you could use the not in operator to say something like size not in small where node affinity will match the node with a size not set to small.)

- We know that we have only set the labels size too large and medium nodes the smaller nodes don't even have the labels set.
- So we don't really have to even check the value of the label as long as we are sure we don't set a label,
size to the smaller nodes using the "Exists" operator will give us the same result.
- The exists the operator will simply check if the label size exists on the notes and you don't need the values section for that as it does not compare the values.

# Node Affinity Types
- the type of node affinity defines the behaviour of the scheduler with respect to node affinity and the stages in the lifecycle of the pod.
- There are currently two types of node affinity available, 
1. required during scheduling ignored during execution
2. preferred during scheduling ignored during execution and 
there are additional types of node affinity,Planned:
 * required during scheduling Required during execution

 "Missed the practice test"

## Resource Requirements & Limits
- Let us look at a 3 Node Kubernetes cluster. Each node has a set of CPU, Memory and Disk resources available.
- Every POD consumes a set of resources.
- In this case 2 CPUs , one Memory and some disk space. Whenever a POD is placed on a Node, it consumes resources available to that node.
- As we have discussed before, it is the kubernetes scheduler that decides which Node a POD goes to. The scheduler takes into consideration, the amount of resources required by a POD and those available on the Nodes.
- In this case, the scheduler schedules a new POD on Node 2. If the node
has no sufficient resources, the scheduler avoids placing the POD on that node Instead places the POD on Node 1 where sufficient resources are available if there is no sufficient resources available on any of the nodes, Kubernetes holds back scheduling the POD, and you will see the POD in a pending state.
- If you look at the events, you will see the reason – insufficient cpu. 
- By default, kubernetes assumes that a POD or a container within a POD requires .5 CPU & 256 Mebibyte of memory.
- This is known as the resource request for a container the minimum amount of CPU or memory requested by the container.
- when the scheduler tries to place the pod on a node.
It uses these numbers to identify a node which has sufficient amount of resources available.
-  if you know that your application will need more than these you can modify these values by specifying them in your pod or deployment definition files:

spec:
  containers:
    resources:
      requests:
      memory: "1Gi"
      cpu: 1

- In this case I set it to 1GB of memory and 1 count of vCPU.
- So what does 1 count of CPU really mean?
* CPU can also be expressed as 100m were m stands for milli.
* You can go as low as 1m, but not lower than that. 1 count of CPU is      equivalent to 1 vCPU.
* That’s 1 vCPU in AWS, or 1 Core in GCP or Azure or 1 Hyperthread. You could request a higher number of CPUs for the container, provided your Nodes are sufficiently funded. 
* Similarly, with memory you could specify 256 Mebibyte using the Mi suffix. Or specify the same value in Memory like this. Or use the suffix G for a gigabyte.
- 1 G (Gigabyte) = 1,000,000,000 bytes
- 1 M (Megabyte) = 1,000,000 bytes
- 1 K (Kilobytes) = 1,0000 bytes
- 1 Gi (Gibibyte) = 1,073,741,824 bytes
- 1 Mi (Mebibyte) = 1,048,576 bytes
- 1 Ki (Kibinyte) = 1,024 bytes
* Note the difference between G and Gi. G is Gigabyte and it refers to a 1000 Megabytes, whereas Gi refers to Gibibyte and refers to 1024 Mebibyte.
- In the docker world a docker container has no limit to the resources it can consume on a Node. 
- Kubernetes sets a limit of 1vCPU to containers. So if you do not specify explicitly, a container will be limited to consume only 1 vCPU from the Node.
- The same goes with memory. By default, kubernetes sets a limit of 512 Mebibyte on containers.
- if you don't like the default limit you can change them by adding a limit section under the resources section in your pod definition file. 

spec:
  containers:
    resources:
      requests:
        memory: "1Gi"
        cpu: 1
      limits:
        memory: "2Gi"
        cpu: 2
- Remember that the limits and requests are set for each container within the pod.
- A container cannot use more CPU resources than its limit.So if a pod tries to consume more memory than its limit constantly, the POD will be terminated.


## Daemon Sets
- Its Use Cases - Lets Say you would like to deploy a monitoring agent or log collector on each of your nodes in the cluster so you can monitor your cluster better.
- Demon set is perfect for that as it can deploy your monitoring agent in the form of a pod in all the nodes in your cluster. Then, you don’t have to worry about adding/removing monitoring agents from these nodes when there are changes in your cluster. as the daemon set will take care of that for you.
- The demon set ensures that one copy of the pod is always present in all nodes in the cluster.
- deamon-set-defination.yaml
<!-- apiVersion: extensions/v1beta1 apps/v1 -->
kind: Deployment
metadata:
  name: jenkins
  labels:
    name: jenkins
    app: jenkins
spec:
  replicas: 1
  selector:
    matchLabels:
      name: jenkins
  template:
    metadata:
      labels:
        app: jenkins
        name: jenkins
      name: jenkins
    spec:
      serviceAccountName: jenkins
      containers:
      - env:
        - name: JAVA_OPTS
          value: -Xmx2048m -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85
        image: jenkins/jenkins #:lts-alpine 
        imagePullPolicy: IfNotPresent

- While discussing the kubernetes architecture we learned that one of the worker node components that is required on every node in the cluster is a kube-proxy.
- That is one good use case of daemon Sets. The kubeproxy component can be deployed as a daemon set in the cluster.
- Another use case is for networking. Networking solutions like weave net requires an agent to be deployed on each node in the cluster.
* How does it work?
- if you were asked to schedule a pod on each node in the cluster.
- how would you do it? 
In one of the previous lectures in this section.

we discussed that we could set the nodeName property on the pod to bypass the scheduler and get the pod placed on a node directly. So that’s one approach. On each pod, set the nodeName property in its specification before it is created and when they are created, they automatically land on the respective nodes.

- So that’s how it used to be until kubernetes version v1.12. From v1.12 onwards the Daemon set uses the default scheduler and node affinity rules that we learned in one of the previous lectures to schedule pods on nodes.

- On how many nodes are the pods scheduled by the DaemonSet kube-proxy

Run the command "kubectl describe daemonset kube-proxy --namespace=kube-system"

## Static Pods
- 
(Its COmpleted but notes need to be done)
## Multiple Schedulers
- view the logs of scheduler : k logs <scheduler-name> --name-space=kube-system
- 

---------------------------------------------------

### Logging Monitoring

# Monitor Cluster Components
- So how do you monitor resource consumption on Kubernetes? Or more importantly what would you like to monitor?
- I’d like to know Node level metrics such as the number of nodes in the cluster, how many of them are healthy as well as performance metrics such as CPU. Memory, network and disk utilization.
- As well as POD level metrics such as the number of PODs, and performance metrics of each POD such as the CPU and Memory consumption on them.
-  Kubernetes does not come with a full featured built-in monitoring solution.
- However, there are a number of open-source solutions available today, such as the Metrics-Server, Prometheus, Elastic Stack, and proprietary solutions like Datadog and Dynatrace.
- Heapster was one of the original projects that enabled monitoring and analysis features for kubernetes
<!-- - <need to update the full notes> -->
<!-- for time being i am wrapping this up -->

# Managing Application Logs
- I could use the docker logs command followed by the container ID. The –f option helps us see the live log trail.
<!-- - Notes Need to be updated -->

### Application Life Cycle Management

# Rolling Updates & Rollbacks
- 

# Commands & Arguments 
- 